% enable page numbering
\pagenumbering{arabic}
%start counting here 
\setcounter{page}{1}
% header for pages
\pagestyle{fancy}
\fancyhead[L]{\myassignment  \hspace*{1.5mm}- \mycourse} 
\fancyhead[R]{\mydate}


%%% STARTE HERE
\section{Question 1 (10 Marks)}

\textit{Given a query that a user submits to an IR system and the top N documents that are returned as relevant by the system, devise an approach (high-level algorithmic steps will suffice)}

\textit{to suggest query terms to add to the query. Typically, we wish to give a large range of suggestions to the users capturing potential intended query needs, i.e., high diversity of terms that may capture the intended query context/content.}


% Ziel: query expanden kÃ¶nnen - which makes the querey better 
% the terms should be relevant (able to expand the query in a meaningful way - find the correct topic), diverse (cover different topics)




% 1. query with q\_n terms, for the whole vocuabulary we get the most similar terms (maybe 1000 t\_n)
% 2. cluster the t\_n terms into k clusters - then we get the most central term from every cluster

% no terms that were in the query 

% how to get the most describtive terms for the query 


% when - runtime vs offline


% there are multiple expansion methods out here: 
% like sysnonm,  related term, contextual, 

% recall vs precision tradeoff what we trying to solve and how we are acutal doing it

% depending what a system we are designing (incooperate user data, e.g. Galway, Ireland, Europe)

% user can give a temperature - to select the cluster

%-------------------------------------------------------------------

% Algorithm for QE suggestions:

% 3. get corresponding clusters for r (all clusters that contain our terms in r + the terms from the query)\\
% 4. select top k clusters ({balancing max distance and max size}) * use multi resolution clustering to be able to tackle different levels of specificity in the query + FORMEL\\
% 5. get one descriptive term for each cluster (e.g. closest to centroid) - source\\
%     -> need to check its not in the original query (if so give the next closed)\\
% 6. return these terms as QE suggestions - number of retuned terms is defined by the number of clusters (k) - like this since we want for our expansion one term from each cluster to sugest for diversity + iterative anyways \\

%-------------------------------------------------------------------

% INTRO
\subsection*{Answer:}

The goal of the here designed query expansion (QE) heuristic is to enhance search results by adding terms that increase the specificity of a query, by proposing both relanvt and diverse terms for the user to select from. The objective is to design a interactive QE technique, by suggesting a range of terms, the user can then add to the query to refine the search results.
Our solution integrates a global concept-based technique with a local pseudo-relevance feedback (PRF), combining relevance and diversity to generate more effective query suggestions \cite{azad2019query}. Our approch is defined in high-level algorithmic steps below in Alorithm~\ref{alg:qe}, and desribed in detail in the following.\\

\textbf{Step 0 (Input and Output):}\\
As prerequisite the document collection is preprocessed to have embeddings and clustering for the terms in the collection. This can be done offline to reduce runtime complexity.
The input to the algorithm is the query terms $q_{terms}$ and the document collection with embeddings and clustering. It has to be noted that during the algorithm we have to embedde the individual query terms as well, so the embedding models has to available.
The output is predifined number of $k$ terms that can be used to expand the original query.\\

\textbf{Step 1 (Query Embedding):}\\
The query needs to be embedded in order to further process it. This is done by embedde the whole query in the same space as the terms in the document collection. In our case we want to find later on terms similar to the whole query instead of just similar to the individual query terms. Therefore we get the embedding for each individual query term and apply mean pooling to get the embedding for the whole query \cite{chen2018enhancing}.\\

\textbf{Step 2 (Get Relevant Terms):}\\
After embedding the query terms, the next step is to extract relevant terms $r_{terms}$. There are two types of relevant terms, the most similar terms from the embedding space and the co-occurrence terms from the top $N$ returned documents on the original query. This is done to ensure that we have semanticly similar terms as well as terms that co-occur with the original query terms.
The semantically similar terms are selected from the embedding space to ensure a search within the entire vocabulary and therefore enhance diversity.
The co-occurrence terms are selected from the top $N$ returned documents on the original query, utilizing a pseudo-feedback (PRF) approach \cite{azad2019query}. This allowed us to add relevant terms that might occur in the same documents but are not semantically similar.
Each of the candiate terms is assigned a weight based on the co-occurrence with the original query terms and then the top $m$ terms are selected as co-occurrence terms \cite{azad2019query}.\\

\textbf{Step 3 (Get Corresponding Clusters):}\\
As we stated in Step 0, the collection vocalbulary has to be clustered. After getting the relevant terms, we can now get the corresponding clusters for these terms. This is done by selecting all clusters that contain the terms in $r_{terms}$.\\

\textbf{Step 4 (Select Top k Clusters):}\\
Since Step 3 can give us back a large number of clusters, we have to select a certain subset of $k$ clusters to extraxt suggesting terms from. We want to select large clusters, since they seem relevant. But the clusters shoudl also be spead out in the embedding space to ensure diversity. 
These two measures are balanced in a score to ranke the clusters as defined by: $$score = \alpha \cdot s + \beta \cdot d$$
where $s$ is the normalised size of the cluster and $d$ is the distance of the cluster centroid to the other cluster centriodes. The parameters $\alpha$ and $\beta$ can be used to adjust the importance of size and distance. Based on this score the top $k$ clusters are selected.\\

\textbf{Step 5 (Extract Terms from Selected Clusters):}\\
The selected top $k$ clusters represent now the most relevant and diverse topics relating to the original query. In order for the user to select one of these topics we have to propose one term that is descriptive for the cluster. 
This is done by selecting the term that is closest to the centroid of the cluster \cite{rossiello2017centroid, khennak2019clustering}.
If this selected terms is already in the original query, the next closest term is selected.\\

\textbf{Step 6 (Return QE Suggestions):}\\
The terms that are selected in Step 5 are then returned as the query expansion suggestions. The number of returned terms is defined by the number of top clusters $k$. The number $k$ is a restriction to ensure that the user has a limited number of terms to select from to be not overwhelmed with suggestions.
\begin{algorithm}
    \caption{Query Expansion (QE) Suggestions}
    \label{alg:qe}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Query terms $q_{terms}$ and document collection with embeddings and clustering
        \State \textbf{Output:} QE suggestion terms
        \State
        
        \State \textbf{Step 1:} Get query embedding from $q_{terms}$ (mean pooling)
        \State \textbf{Step 2:} Get $r_{terms}$ relevant terms (union of $s_{terms}$ and $c_{terms}$)
        \State \quad \textbf{Step 2.1:} Get $n$ most similar terms from embedding space ($s_{terms}$)
        \State \quad \textbf{Step 2.2:} Get $m$ co-occurrence terms from top $N$ returned documents on the original query  ($c_{terms}$)
        \State \textbf{Step 3:} Get corresponding clusters for $r_{terms}$
        \State \textbf{Step 4:} Select top $k$ clusters (weighted with size and centriod distance)
        % \State \quad \textbf{Step 4.1:} Use multi-resolution clustering to address different levels of specificity in the query
        \State \textbf{Step 5:} Get one descriptive term for each cluster (e.g., closest to centroid)
        \State \quad \textbf{Step 5.1:} Check that the descriptive term is not in the original query
        \State \textbf{Step 6:} Return the descriptive terms as QE suggestions
    \end{algorithmic}
\end{algorithm}

\textbf{Reflection:}\\
By precomputing the vocabulary embedding and clustering as well as the embedding model for the query runtime complexity is significantly reduced. Which allowes for a fast response time for the user. 

In the case our query and the resulting releveant terms are so specidify that we only get one highlevel cluster (e.g., ``jaguar engine horsepower" $\rightarrow$ cluster: ``cars"). In that case we might want lowlevel clusters like the different engines or jaguar models. As a solution instead of just precomputing one clustering, multi-resolution clustering can be used to address different levels of specificity in the query \cite{lutov2019accuracy}. The level of specificity would need to be seleted during runtime wihtin our algorithm before Step 3.

In general a lot of hyperparamters in our algorithm can be adjusted to the specific use case and also need to be examined in general studies to find the best values for them. Possible tunable parameters are, the number of similar terms $n$ and co-occurrence terms $m$ to select, the number of clusters $k$ to select, the weighting parameters $\alpha$ and $\beta$ for the cluster selection, and the level of specificity for the possible multi-resolution clustering.


\newpage
\section{Question 2 (10 Marks)}

\textit{Consider the following scenario: a company search engine is employed to allow people to search a large repository. All queries submitted to the system are recorded. A record that contains the id of the user and the terms in the query is stored. The order of the terms is not stored and neither is any timestamp. Each entry in this record is effectively an id and a set of terms.}

\textit{The designers of the search engine, decide to use this information to develop an approach to make query term suggestions for users, i.e., at run time, once a user has entered their query terms, the system will suggest potential extra terms to add to the query.}

\textit{Given the data available, outline an approach that could be adopted to generate these suggested terms. A brief outline is sufficient to capture the main ideas in your approach. The designers of the system wish to take into account previous queries and any similarities between users. Identify the advantages and disadvantages of your approach (briefly).}

\subsection*{Answer:}

The goal is to develop a system that suggests additional terms for an entered query $q$ based on the recorded history of user queries $H$ (log). Our approach incorporates three levels of scoring. 

In the whole vocabulary we have candiate terms, that we might want to suggest to the user. We rank these termes based on a score that consists of three components, one based on the users own search history, one based on the search history of similar users and one based on the global set of all queries. By adjusting the weights of these components the score of the term can be adjusted.
The score is defined as: 
$$score(t) = \alpha \cdot user\_h\_score(t) + \beta \cdot similar\_u\_score(t) + \gamma \cdot global\_h\_score(t)$$ Where $\alpha$, $\beta$ and $\gamma$ are the weights for the three components. 
The three subscores are based on the frequency a term occurs given the entered query $q$. This means for the $user\_h\_score(t)$ a co-occurence score is calculated based on how often the term $t$ occurs with the terms in the query $q$ in the users history. The same goes for the other two scores in their respective context (similar users and global history).

While the user history and global history are clearly defined, similar users need to be defined as such in order to generate the context (queries from similar users).
We consider two users to be similar if they have a high similarity in their query history. For simplicity, this is calculated as the average query similarity between two users, given by: $$\text{UserSimilarity}(A, B) = \frac{\sum_{i=1}^m \sum_{j=1}^n \cdot \text{QuerySimilarity}(q_{Ai}, q_{Bj})}{ m \cdot n}$$
The $\text{QuerySimilarity}(q_{Ai}, q_{Bj})$ function is used to calculate the similarity between two queries and can be calculated by applying metrics like the Jaccard similarity or cosine similarity on the query representations.
A threshold can be set to define when two users are considered similar. This results in a set of similar users for each user, which can be used to calculate the $similar\_u\_score(t)$. The $similar\_u\_score(t)$ works analogous to the $user\_h\_score(t)$ but is based on the query history of the similar users.

Based on the defined score, all terms in the vocabulary can be ranked, and the top $k$ terms can be suggested to the user. The weights $\alpha$, $\beta$, and $\gamma$ can be adjusted to the specific use case to give more weight to the user's history, similar users, or the global history. This weighting can also be adjusted dynamically based avaibale data. This allowes for relvant suggesting even for new users or users with very few similar users, by giving more weight to the global history. This third component adds a global context to the term score and depending on its weight can give `new to the user' terms a chance to be suggested. It also provides a fallback for the case that a user enters unusual terms that are not in the history of the user or similar users or the case of a user being new and having no or very limited history.

Since the third component (global history) encompasses the first and second, occurrences in the user's own history and similar users' history receive an additional boost. this can be handled by excluding the user's own history and similar users' history from the global history. But its not necessary since we assume that these components are more relanvant and the boost has no negative influence on the performance.

We chose grouping similar users together based on their average query similarity, to differentiate between two users using the same terms over all query and two users using the same terms in the same combinations. We weight users as similar who use the same terms in the same combinations, as they are more likely to have similar interests and therefore similar queries. It would also be possible to cluster users based on their query history.\\

\textbf{Advantages:}
\begin{itemize}
    \item \textbf{Adaptability:} Weights can be adjusted based on use-case.
    \item \textbf{Gobal Aspect:} Does not require extensive user history to provide suggestions (user independent component).
    \item \textbf{Scalability:} User similarity can be calculated offline and stored for quick access.
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
    \item \textbf{Cold Start for New Terms:} Rare or novel terms may not be suggested effectively, as they may not appear frequently in the dataset (gobal and user based).
    \item \textbf{Out-dated History:} Inheriting old search history and user profiles can lead to entirely irrelevant suggestions, for example, if a user changes their position within the company.
\end{itemize}
